---
title: "Regression and Analysis of Variance"
author: "Dataset 3 (Fish Market)."
output:
  word_document:
    toc: yes
  pdf_document:
    fig_caption: yes
    latex_engine: xelatex
    toc: yes
urlcolor: blue
linkcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

```{r, echo=FALSE}
# Loading library for analysis
library(psych)
library(ggplot2)
library(table1)
library(tidyverse) # metapackage of all tidyverse packages
library(ggpubr) # violin plot to visualize weight distribution per species
library(viridis) # color palette
library(wesanderson) # color palette
library(ggthemes) # theme layer
library(corrplot) # plotting pairwise correlation plot matrix
library(WVPlots) # gain plot
library(naniar) # visualize NAs
library(broom) # tidy model output
```

## Fish Market Dataset - Data Description

The fish market dataset described below contains information on common species in fish market sales; as well as its weight, length, height and width.

__Loading Data__

The csv will be loaded as a dataframe to verify the structure of the dataset.

```{r}
data = read.csv(file='Fish.csv', col.names = c("Species","Weight", "Length_V","Length_D", 
                                               "Length_C", "Height", "Width"))
head(data)
```
**New additional data point**

```{r}
new_additional_data_point<- data.frame("Species" = c('Whitefish'), "Weight" = c(400),
                                       "Length_V"= c(29.35),"Length_D"= c(30.05),
                                       "Length_C"= c(32.31), "Height"= c(9.300), 
                                       "Width"= c(5.203))
head(new_additional_data_point)
```
This single data is chosen preserving the distribution of each variable according to the type of species. In this case, a data point was added following the distribution of the variables for the Whitefish species.

To add the new_additional_data_point to the fish market dataset, simply do the following:

```{r}
data <- rbind(data,new_additional_data_point)
data$Species= as.factor(data$Species)
```

__Variable names:__

_1. Species:_ The name of the fish species.

_2. Weight:_ Weight of fish in grams.

_3. Length_V:_ Vertical length of fish in centimeters.

_4. Length_D:_ Diagonal length of fish in centimeters.

_5. Length_C:_ Cross length of fish in centimeters..

_6. Height:_ Fish height in centimeters.

_7. Width:_ Diagonal width of the fish in centimeters.

```{r}
dim(data)
```
There are 160 records with 7 variables to analyze.

__Target Exploration__

The purpose of the study is to determine the weight of the fish, relating the type of species, the length, the height and its width. So, essentially we have 6 features and 1 target column which is weigth.


## Exploratory Data Analysis

The general idea is to analyze whether a multiple linear regression model can be fitted, where the response variable can be explained from more than one explanatory variable.

**Response variable:** Weigth.

**Explanatory variables:** Species, Length_V, Length_D, Length_C, Height, Width.

It is necessary to make a descriptive analysis of the dataset to determine if a regression model can be adjusted, for this various statistical techniques are applied.

__Statistical Summary of the Fish Market Dataset__

Using the following instruction in R a statistical summary is generated for the Fish Market Dataset.

```{r data}
data$Species=as.factor(data$Species)
summary(data)

```

Some key observations are listed:

*1.* There is a combination of categorical and numeric variables.

*2.* The Whitefish species only has 7 observations.

*3.* The Perch species has the highest number of observations.

*4.* There are outliers at the extremes of the Weight variable, the minimum value is $0.0$ and the maximum value is $1650$.

*5.* The vertical length and diagonal length of the fish have similar characteristics.

*6.* Some biases towards the right are observed in the numerical variables, this property is observed when comparing the medians and means of the data.

**Univariate Analysis - Categorical Features**

**Species**

Let's look at the categorical variable and verify the distribution

```{r fig.width=6, fig.height=3, fig.align = "center"}
order <- names(sort(table(data$Species), decreasing=TRUE))
sample<- data.frame(value=factor(data$Species, levels=order))
barplot(table(sample),col=c("steelblue"),main="Species")

```


There are 7 different species of fish to analyze, the most commercialized are the Perch species followed by Bream.

**Univariate Analysis - Numeric Features**

**Weight**

The boxplot and histogram are plotted for the weight variable.

```{r fig.width=6, fig.height=3.5, fig.align = "center"}
par(mfrow = c(1,2))
boxplot(data$Weight,main="Boxplot for Weight",col=c("steelblue"),xlab='',ylab="Weigth")
hist(data$Weight,freq = F ,main='Histogram for Weight',xlab='Weight',col = "steelblue")
lines(density(data$Weight))
```

Observations that are between $50\%$ and $75\%$ are more dispersed than between $25\%$ and $50\%$. Outliers are observed.

The distribution of the data is skewed to the right, some transformation must be made to the weight variable.

We plot the boxplots and histograms of the other variables.

**Length_V**

```{r fig.width=6, fig.height=3.5, fig.align = "center"}
par(mfrow = c(1,2))
boxplot(data$Length_V,main="Boxplot for Length_V",col=c("steelblue"),xlab='',ylab="Length_V")
hist(data$Length_V,freq = F ,main='Histogram for Length_V',xlab='Length_V',col = "steelblue")
lines(density(data$Length_V))
```

**Length_D**

```{r fig.width=6, fig.height=3.5, fig.align = "center"}
par(mfrow = c(1,2))
boxplot(data$Length_D,main="Boxplot for Length_D",col=c("steelblue"),xlab='',ylab="Length_D")
hist(data$Length_D,freq = F ,main='Histogram for Length_D',xlab='Length_D',col = "steelblue")
lines(density(data$Length_D))
```

**Length_C**

```{r fig.width=6, fig.height=3.5, fig.align = "center"}
par(mfrow = c(1,2))
boxplot(data$Length_C,main="Boxplot for Length_C",col=c("steelblue"),xlab='',ylab="Length_C")
hist(data$Length_C,freq = F ,main='Histogram for Length_C',xlab='Length_C',col = "steelblue")
lines(density(data$Length_C))
```

**Height**

```{r fig.width=6, fig.height=3.2, fig.align = "center"}
par(mfrow = c(1,2))
boxplot(data$Height,main="Boxplot for Height",col=c("steelblue"),xlab='',ylab="Height")
hist(data$Height,freq = F ,main='Histogram for Height',xlab='Height',col = "steelblue")
lines(density(data$Height))
```

**Width**

```{r fig.width=6, fig.height=3.5, fig.align = "center"}
par(mfrow = c(1,2))
boxplot(data$Width,main="Boxplot for Width",col=c("steelblue"),xlab='',ylab="Width")
hist(data$Width,freq = F ,main='Histogram for Width',xlab='Width',col = "steelblue")
lines(density(data$Width))
```

Outliers are observed in lenght_V, lenght_D, Lenght_C, it is advisable to transform the data for a better fit of the model.

**Cleaning and transformation of Data**

Observations with values equal to 0 and outliers determined in the boxplots must be eliminated from this analysis.

**Weight-Removing outliers**

```{r fig.width=6, fig.height=4, fig.align = "center"}
par(mfrow = c(1,1))
Quantil_Weigth<-quantile(data$Weight, c(0.25, 0.5, 0.75), type = 7)
print(Quantil_Weigth)
IQR.Weigth<-IQR(data$Weight)
outliers_max_Weigth<-as.numeric(Quantil_Weigth[3])+1.5*IQR.Weigth
print(outliers_max_Weigth)
outliers_min_Weigth<-as.numeric(Quantil_Weigth[1])-1.5*IQR.Weigth
print(outliers_min_Weigth)
boxplot(sort(data$Weight[data$Weight>outliers_min_Weigth & data$Weight<outliers_max_Weigth],
             decreasing = FALSE),main="Boxplot",
        col=c("steelblue"),
        xlab="",
        ylab="Weigth")
```

**Length_V-Removing outliers**

```{r fig.width=6, fig.height=4, fig.align = "center"}
par(mfrow = c(1,1))
Quantil_Length_V<-quantile(data$Length_V, c(0.25, 0.5, 0.75), type = 7)
print(Quantil_Length_V)
IQR.Length_V<-IQR(data$Length_V)
outliers_max_Length_V<-as.numeric(Quantil_Length_V[3])+1.5*IQR.Length_V
print(outliers_max_Length_V)
outliers_min_Length_V<-as.numeric(Quantil_Length_V[1])-1.5*IQR.Length_V
print(outliers_min_Length_V)

boxplot(sort(data$Length_V[data$Length_V>outliers_min_Length_V &
        data$Length_V<outliers_max_Length_V],decreasing = FALSE),
        main="Boxplot", col=c("steelblue"),xlab="",ylab="Length_V")
```

**Length_D-Removing outliers**

```{r fig.width=6, fig.height=4, fig.align = "center"}
par(mfrow = c(1,1))
Quantil_Length_D<-quantile(data$Length_D, c(0.25, 0.5, 0.75), type = 7)
print(Quantil_Length_D)
IQR.Length_D<-IQR(data$Length_D)
outliers_max_Length_D<-as.numeric(Quantil_Length_D[3])+1.5*IQR.Length_D
print(outliers_max_Length_D)
outliers_min_Length_D<-as.numeric(Quantil_Length_D[1])-1.5*IQR.Length_D
print(outliers_min_Length_D)
boxplot(sort(data$Length_D[data$Length_D>outliers_min_Length_D & 
             data$Length_D<outliers_max_Length_D],decreasing = FALSE),main="Boxplot",
             col=c("steelblue"),xlab="",ylab="Length_D")
```

**Length_C-Removing outliers**

```{r fig.width=6, fig.height=4, fig.align = "center"}
par(mfrow = c(1,1))
Quantil_Length_C<-quantile(data$Length_C, c(0.25, 0.5, 0.75), type = 7)
print(Quantil_Length_C)
IQR.Length_C<-IQR(data$Length_C)
outliers_max_Length_C<-as.numeric(Quantil_Length_C[3])+1.5*IQR.Length_C
print(outliers_max_Length_C)
outliers_min_Length_C<-as.numeric(Quantil_Length_C[1])-1.5*IQR.Length_C
print(outliers_min_Length_C)
boxplot(sort(data$Length_C[data$Length_C>outliers_min_Length_C 
        & data$Length_C<outliers_max_Length_C],decreasing = FALSE),main="Boxplot",
        col=c("steelblue"), xlab="",ylab="Length_C")
```

The new filtered database is saved.


```{r}
data2<- data.frame(data[data$Weight>0 & data$Weight>outliers_min_Weigth 
                  & data$Weight<outliers_max_Weigth & data$Length_V>outliers_min_Length_V
                  & data$Length_V<outliers_max_Length_V& data$Length_D>outliers_min_Length_D
                  & data$Length_D<outliers_max_Length_D & data$Length_C>outliers_min_Length_C
                  & data$Length_C<outliers_max_Length_C,])
```
A logarithmic transformation is applied to the data for a better fit of the model.

```{r}
data2=cbind('Species'=data2$Species,log(data2[,-1]))
dim(data2)
```


There are 156 records with 7 variables to analyze.


**Bivariate Analysis - Categorical Features**

**Species**

```{r fig.width=8, fig.height=4, fig.align = "center", echo=FALSE}
boxplot(data2$Weight~data2$Species,main="Boxplot",col=c("steelblue"),xlab="Species",ylab="Weigth")
```

Outliers are observed in the distribution of Roach species in relation to weight. The average weight in each species has significant variations.

```{r fig.width=4, fig.height=4, fig.align = "center"}

pairs.panels(data2[,-1],method = "pearson",hist.col = "steelblue",
             density = TRUE,ellipses = TRUE)
```

The data scatter plot shows a relationship between the variables analyzed, in some cases it is linear, in others the relationship is not linear.However, when comparing the explanatory variables with the response variable, a linear relationship can be seen.

The linear correlation coefficient between the analyzed variables is positive and very close to one, which confirms the information provided by the scatterplot, where a clear linear relationship is observed between the variables with a positive relationship.

A high positive linear relationship can be observed between the weight variable and the width variable $(cor = 0.98)$.
There is also a strong linear relationship between weight and Lenght_C of 0.97. 
 
To avoid problems of multicollinearity between the explanatory variables, the highly correlated explanatory variables will be eliminated, leaving the variable with the highest correlation with the response variable. In that sense,the Length_V Length_D Length_C variables are correlated with each other.

The Length_V and Length_D variables will be removed.

Next, the scatter plots will be shown in greater detail about the species.

```{r fig.width=6, fig.height=6, fig.align = "center"}
Sc1<- ggplot(data2, aes(x = Width, y = Weight)) +
        geom_point(aes(color = Species), size = 3) +
        geom_smooth(method = "lm", se = FALSE, color = "black") +
        labs(title = "Scatterplot Width vs. Weight by Species") +
        scale_color_viridis_d() +
        theme(plot.title = element_text(size=14),
              axis.text.x= element_text(size=12),
              axis.text.y= element_text(size=12), 
              axis.title=element_text(size=12),
              legend.title = element_text(size = 12),
             legend.text = element_text(size = 12))
Sc2<- ggplot(data2, aes(x = Height, y = Weight)) +
        geom_point(aes(color = Species), size = 3) +
        geom_smooth(method = "lm", se = FALSE, color = "black") +
        labs(title = "Scatterplot Height vs. Weight by Species") +
        scale_color_viridis_d() +
        theme(plot.title = element_text(size=14),
              axis.text.x= element_text(size=12),
              axis.text.y= element_text(size=12), 
              axis.title=element_text(size=12),
              legend.title = element_text(size = 12),
             legend.text = element_text(size = 12))
Sc3<- ggplot(data2, aes(x = Length_C, y = Weight)) +
        geom_point(aes(color = Species), size = 3) +
        geom_smooth(method = "lm", se = FALSE, color = "black") +
        labs(title = "Scatterplot Length_C vs. Weight by Species") +
        scale_color_viridis_d() +
        theme(plot.title = element_text(size=14),
              axis.text.x= element_text(size=12),
              axis.text.y= element_text(size=12), 
              axis.title=element_text(size=12),
              legend.title = element_text(size = 12),
             legend.text = element_text(size = 12))
ggarrange(Sc1,Sc2,Sc3,ncol = 1, nrow = 3)
```

Let's see if the correlation between the variable weigth and Length_C is significant.

$H_{0}:\rho=0$ 

$H_{1}:\rho \neq 0$ 

```{r}
cor.test(data2$Weight,data2$Length_C)
```

With a $p-value= 2.2e-16<0.05$, H0 is rejected, there is a linear relationship between Weigth and Length_C. The correlation is positive with a value very close to one.
```{r}
cor.test(data2$Weight,data2$Height)
```
```{r}
cor.test(data2$Weight,data2$Width)
```
The same result is observed when applying the correlation tests between variables to Height and Width and Weight.

With a $p-value= 2.2e-16<0.05$, H0 is rejected, there is a linear relationship between Weigth and Height. $(cor=0.93)$ close to one.

With a $p-value= 2.2e-16<0.05$, H0 is rejected, there is a linear relationship between Weigth and Width. $(cor=0.98)$ close to one.


There are the conditions to perform a fit of a linear regression model


## Fitting a Multiple Linear Regression Model

Before performing the model fit, dummy variables will be created from the species variables.

```{r}
library(fastDummies)
results <- fastDummies::dummy_cols(data2)
data2=as.data.frame(results)
```

```{r}
library(olsrr)
library(ggplot2)
library(gridExtra)
library(nortest)
library(goftest)
model=lm(data2$Weight~data2$Species_Bream+data2$Species_Parkki+data2$Species_Perch+
        data2$Species_Pike+data2$Species_Roach+data2$Species_Smelt+data2$Species_Whitefish+
        data2$Length_C+data2$Height+data2$Width)
summary(model)
ols_step_forward_aic(model)
```

With p-values< 0.05, the Species_Bream Species_Parkki Species_Roach Species_Smelt Length_C Height Width are significant in the fit of the linear regression model.

The selection of variables to adjust the model was carried out using the Akaike information criterion (AIC), selecting those with the lowest value in their AIC, in addition those with a significant p-value were chosen.

**Fitted Model**

```{r}
m1=lm(data2$Weight~data2$Species_Bream+data2$Species_Parkki+data2$Species_Smelt+
        data2$Length_C+data2$Height)
summary(m1)
```

 **Interpretation of estimates**
 
*1.-* $\beta_{0}=-2.90064:$ It is estimated that the average weight decreases -2.90064 when all the variables.

*2.-* $\beta_{1}= -0.52447:$ It is estimated that by an increase in Bream Species, the weight could decreases 0.52447.

*3.-* $\beta_{2}= -0.36839:$ It is estimated that by an increase in Parkki Species, the weight could decreases 0.36839.

*4.-* $\beta_{3}= -0.14836:$ It is estimated that by an increase in Smelt Species, the weight could decreases 0.14836.

*5.-* $\beta_{4}= 1.70962:$ It is estimated that by an increase in Length$\_$C, the weight increases 1.70962.

*6.-* $\beta_{5}= 1.31482:$ It is estimated that by an increase in Height, the weight increases 1.31482.

The Wald tests are used to test whether there is a linear relationship between each of the explanatory variables and the response variable, in the presence of the other independent variables.

__Hypotheses of the Wald test__

$H_{0}:\beta_{j}$

$H_{1}:\beta_{j} \neq 0$ 

With $p-values< 0.05$, all the variables used in the model fit are significant.The residual error of the model is $0.1048$ with 150 degrees of freedom.The $R^{2}= 0.99$, the adjusted model explains $99\%$ of the variability of the data.

The linear equation represented by the model is the following:

$log(Weigth)= (-2.90064)+(-0.52447)*Bream + (-0.36839)*Parkki + (-0.14836)*Smelt +
(1.70962)*log(Length\_C) + (1.31482)*log(Height)$

## Variance Decomposition

A measure of how good a model is for fitting some data is to quantify how much of the variability contained in them has been explained by said model. A model is good if the explained variability is high, or what is the same, if the differences between the data and the predictions are small according to the model.The goodness of fit statistic of the regression is based on comparing the variability explained by the model with that which remains unexplained, that is, in the quotient of the sums of mean squares $MSE$ and $MSR$, which turns out to have an $F$ distribution with $1$ and $n-2$ degrees of freedom when the model is correct.

Testing the goodness of fit of the regression line means solving the contrast:

$H_{0}:\beta_{j}=0$   

$H_{1}:$At least one $\beta_{j} \neq 0$ 

```{r}
anova_m1=na.omit(anova(m1))
anova_m1
```

With $p-values<0.05$, H0 is rejected, that is, there is a relationship linear between the weight and at least one of the explanatory variables of the model.

When comparing the adjustment results of the m1 model and the anova analysis in terms of statistical significance of the variables; there are no differences.

Let's see how the estimates of the global model differ with Lenght_C, Height, Species_Bream, Species_Parkki, Species_Smelt  from the simple linear regression models that we can build with each of these explanatory variables:


```{r}
m2=lm(data2$Weight~data2$Length_C)
summary(m2)
anova_m2=na.omit(anova(m2))
anova_m2
```

The coefficient values differ from the global estimate, Multiple R-squared:  0.95 does not differ much from Multiple R-squared global 0.99

When comparing the adjustment results of the m1 model and the anova analysis for m2 model in terms of statistical significance of the variables; there are no differences.Similar results were observed when performing the analysis for the other variables.

```{r}
m3=lm(data2$Weight~data2$Height)
summary(m3)
anova_m3=na.omit(anova(m3))
anova_m3
```

```{r}
m4=lm(data2$Weight~data2$Species_Bream)
summary(m4)
anova_m4=na.omit(anova(m4))
anova_m4
```

```{r}
m5=lm(data2$Weight~data2$Species_Parkki)
summary(m5)
anova_m5=na.omit(anova(m5))
anova_m5
```
```{r}
m6=lm(data2$Weight~data2$Species_Smelt)
summary(m6)
anova_m6=na.omit(anova(m6))
anova_m6
```

In general, the hypothesis contrast tests generate favorable results for the estimates despite the fact that their Multiple R-squared describes some variability in the data, which allows us to say that the variables proposed to explain the response variable are the most appropriate.

## Residual Analysis of Model Ajusted

An analysis of the residuals is performed to decide if the fitted model is correct or if a change in the variables is needed to generate a better model.

```{r fig.width=4, fig.height=4, fig.align = "center"}
plot(residuals(m1),main='Scatterplot Residuals Model Ajusted',ylab='Residuals')
```

Dispersion is observed in the residuals of the fitted model.

```{r fig.width=3.5, fig.height=3.5,fig.align = "center"}
qqnorm(m1$residuals)
qqline(m1$residuals)
hist(resid(m1),col = "steelblue",main='Histogram Residuals Model',xlab='Residuals')
lines(density(residuals(m1)))
```

When analyzing the QQ-plot, it is observed that the residuals conform to a normal distribution.

The histogram for the model residuals shows a normal distribution in the observations.

**Normality Test Residuals Model**

$H_{0}:$ The residuals have a normal distribution.

$H_{1}:$ The residuals have no normal distribution. 


```{r}
shapiro.test(residuals(m1))
    ```

Since $p-value = 0.2581< 0.05$ the hypothes null is not rejected, then the residuals have no normal distribution. 

In conclusion, the given multiple regression model fits the data satisfactorily, this is observed when performing the normality test of the residuals.